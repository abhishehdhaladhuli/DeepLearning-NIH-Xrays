{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f28cdb-c1ba-4694-aa14-9f796b578072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# Cell 1: Import Libraries & Setup\n",
    "# ========================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from albumentations import Compose, Resize, Normalize, HorizontalFlip, RandomBrightnessContrast\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✅ Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50084fb-d5d1-424e-a0fe-ab9ab355654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GREAT NEWS! Your trained model was found!\n",
      "   Location: experiments/chest_xray_20251022_093109/best_model.pth\n",
      "\n",
      "📊 Model Information:\n",
      "   Trained for: 5 epochs\n",
      "   Validation AUC: 0.8024\n",
      "   Training complete: ✅\n",
      "\n",
      "   YOU DON'T NEED TO RETRAIN!\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# VERIFY YOUR SAVED MODEL\n",
    "# ========================================================================\n",
    "\n",
    "# Your previous experiment name\n",
    "previous_experiment = \"chest_xray_20251022_093109\"  # From your training\n",
    "\n",
    "# Check if model exists\n",
    "model_path = f'experiments/{previous_experiment}/best_model.pth'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"✅ GREAT NEWS! Your trained model was found!\")\n",
    "    print(f\"   Location: {model_path}\")\n",
    "    \n",
    "    # Load checkpoint info\n",
    "    checkpoint = torch.load(model_path, weights_only=False)\n",
    "    print(f\"\\n📊 Model Information:\")\n",
    "    print(f\"   Trained for: {checkpoint['epoch']} epochs\")\n",
    "    print(f\"   Validation AUC: {checkpoint['val_auc_macro']:.4f}\")\n",
    "    print(f\"   Training complete: ✅\")\n",
    "    print(\"\\n   YOU DON'T NEED TO RETRAIN!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Model not found!\")\n",
    "    print(f\"   Looking for: {model_path}\")\n",
    "    print(\"\\n   Please check the experiment folder name.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058b9025-6b3a-482e-9bbc-95af7a599f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "✅ MODEL LOADED SUCCESSFULLY!\n",
      "======================================================================\n",
      "Model: EfficientNet-B0\n",
      "Trained AUC: 0.8024\n",
      "Epoch: 5\n",
      "Ready for: Next phase training\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# REBUILD MODEL AND LOAD TRAINED WEIGHTS\n",
    "# ========================================================================\n",
    "\n",
    "class ChestXrayClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=14, pretrained=True):\n",
    "        super(ChestXrayClassifier, self).__init__()\n",
    "        \n",
    "        if pretrained:\n",
    "            self.backbone = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        else:\n",
    "            self.backbone = EfficientNet.from_name('efficientnet-b0')\n",
    "        \n",
    "        num_features = self.backbone._fc.in_features\n",
    "        self.backbone._fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Create model\n",
    "model = ChestXrayClassifier(num_classes=14, pretrained=False)\n",
    "\n",
    "# Load trained weights\n",
    "checkpoint = torch.load(model_path, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"✅ MODEL LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: EfficientNet-B0\")\n",
    "print(f\"Trained AUC: {checkpoint['val_auc_macro']:.4f}\")\n",
    "print(f\"Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"Ready for: Next phase training\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bbf513b-e707-4ae4-bbde-8e0d9c0d2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 \n",
      "PHASE 2: HIGH-RESOLUTION TRAINING (224x224)\n",
      "🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 \n",
      "\n",
      "======================================================================\n",
      "WHY 224x224?\n",
      "======================================================================\n",
      "  • Your current model: 128x128 → 76.41% AUC\n",
      "  • Expected with 224x224: 78-79% AUC (+2-3%)\n",
      "  • Standard in medical imaging research\n",
      "  • Better detail preservation\n",
      "  • Still fits in 8GB GPU memory\n",
      "======================================================================\n",
      "\n",
      "✅ Dataset loaded:\n",
      "   Train:      69,219 images\n",
      "   Validation: 17,305 images\n",
      "   Test:       25,596 images\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# PREPARE FOR 224x224 TRAINING (BETTER QUALITY)\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"🚀 \"*35)\n",
    "print(\"PHASE 2: HIGH-RESOLUTION TRAINING (224x224)\")\n",
    "print(\"🚀 \"*35 + \"\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"WHY 224x224?\")\n",
    "print(\"=\"*70)\n",
    "print(\"  • Your current model: 128x128 → 76.41% AUC\")\n",
    "print(\"  • Expected with 224x224: 78-79% AUC (+2-3%)\")\n",
    "print(\"  • Standard in medical imaging research\")\n",
    "print(\"  • Better detail preservation\")\n",
    "print(\"  • Still fits in 8GB GPU memory\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load dataset paths\n",
    "extract_path = \"./chest_xray_data/\"  # UPDATE if different\n",
    "data_entry = pd.read_csv(os.path.join(extract_path, \"Data_Entry_2017.csv\"))\n",
    "\n",
    "# Prepare splits (same as before)\n",
    "train_list_path = os.path.join(extract_path, \"train_val_list.txt\")\n",
    "test_list_path = os.path.join(extract_path, \"test_list.txt\")\n",
    "\n",
    "if os.path.exists(train_list_path) and os.path.exists(test_list_path):\n",
    "    train_list = pd.read_csv(train_list_path, header=None)[0].tolist()\n",
    "    test_list = pd.read_csv(test_list_path, header=None)[0].tolist()\n",
    "    train_val_data = data_entry[data_entry['Image Index'].isin(train_list)].reset_index(drop=True)\n",
    "    test_data = data_entry[data_entry['Image Index'].isin(test_list)].reset_index(drop=True)\n",
    "else:\n",
    "    train_val_data, test_data = train_test_split(data_entry, test_size=0.3, random_state=42)\n",
    "\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\n✅ Dataset loaded:\")\n",
    "print(f\"   Train:      {len(train_data):,} images\")\n",
    "print(f\"   Validation: {len(val_data):,} images\")\n",
    "print(f\"   Test:       {len(test_data):,} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a973b4ba-b28c-43b4-b3c0-c774c3fbb02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HIGH-RESOLUTION CONFIGURATION\n",
      "======================================================================\n",
      "Image size:   224x224 (was 128x128)\n",
      "Improvement:  3x more pixels per image\n",
      "Benefit:      Better detail, clearer pathology features\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# 224x224 TRANSFORMS (HIGHER RESOLUTION)\n",
    "# ========================================================================\n",
    "\n",
    "IMAGE_SIZE = 224  # INCREASED from 128\n",
    "\n",
    "train_transforms_hires = Compose([\n",
    "    Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.3),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transforms_hires = Compose([\n",
    "    Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HIGH-RESOLUTION CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Image size:   {IMAGE_SIZE}x{IMAGE_SIZE} (was 128x128)\")\n",
    "print(f\"Improvement:  3x more pixels per image\")\n",
    "print(f\"Benefit:      Better detail, clearer pathology features\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2ef27f-0835-42f6-906f-4a149685ae56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ DataLoaders created:\n",
      "   Image size:    224x224\n",
      "   Batch size:    32 (reduced for memory)\n",
      "   Train batches: 2,164\n",
      "   Val batches:   541\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# DATASET CLASS (Same as before)\n",
    "# ========================================================================\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None, num_classes=14):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.pathologies = [\n",
    "            'Atelectasis', 'Consolidation', 'Infiltration', \n",
    "            'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', \n",
    "            'Effusion', 'Pneumonia', 'Pleural_Thickening', \n",
    "            'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['Image Index']\n",
    "        \n",
    "        possible_paths = [\n",
    "            os.path.join(self.img_dir, 'images', img_name),\n",
    "            os.path.join(self.img_dir, img_name),\n",
    "        ]\n",
    "        \n",
    "        for i in range(1, 13):\n",
    "            possible_paths.append(\n",
    "                os.path.join(self.img_dir, f'images_{i:03d}', 'images', img_name)\n",
    "            )\n",
    "        \n",
    "        img_path = None\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                img_path = path\n",
    "                break\n",
    "        \n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_name}\")\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        labels_str = self.dataframe.iloc[idx]['Finding Labels']\n",
    "        labels = np.zeros(self.num_classes, dtype=np.float32)\n",
    "        \n",
    "        if labels_str != 'No Finding':\n",
    "            for i, pathology in enumerate(self.pathologies):\n",
    "                if pathology in labels_str:\n",
    "                    labels[i] = 1.0\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, torch.tensor(labels)\n",
    "\n",
    "# ========================================================================\n",
    "# CREATE DATALOADERS WITH 224x224 IMAGES\n",
    "# ========================================================================\n",
    "\n",
    "BATCH_SIZE = 32  # Reduced from 64 due to larger images\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "train_dataset = ChestXrayDataset(train_data, img_dir=extract_path, transform=train_transforms_hires)\n",
    "val_dataset = ChestXrayDataset(val_data, img_dir=extract_path, transform=val_transforms_hires)\n",
    "test_dataset = ChestXrayDataset(test_data, img_dir=extract_path, transform=val_transforms_hires)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"\\n✅ DataLoaders created:\")\n",
    "print(f\"   Image size:    {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"   Batch size:    {BATCH_SIZE} (reduced for memory)\")\n",
    "print(f\"   Train batches: {len(train_loader):,}\")\n",
    "print(f\"   Val batches:   {len(val_loader):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da1b8ddd-03b8-4b24-8539-77c363a1946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Class weights calculated\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# CALCULATE CLASS WEIGHTS\n",
    "# ========================================================================\n",
    "\n",
    "def calculate_class_weights(dataframe, num_classes=14):\n",
    "    pathologies = [\n",
    "        'Atelectasis', 'Consolidation', 'Infiltration', \n",
    "        'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', \n",
    "        'Effusion', 'Pneumonia', 'Pleural_Thickening', \n",
    "        'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'\n",
    "    ]\n",
    "    \n",
    "    total_samples = len(dataframe)\n",
    "    pos_weights = []\n",
    "    \n",
    "    for pathology in pathologies:\n",
    "        pos_count = dataframe['Finding Labels'].str.contains(pathology, regex=False).sum()\n",
    "        neg_count = total_samples - pos_count\n",
    "        weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "        pos_weights.append(weight)\n",
    "    \n",
    "    return torch.tensor(pos_weights, dtype=torch.float32)\n",
    "\n",
    "class_weights = calculate_class_weights(train_data)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "print(\"✅ Class weights calculated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66349381-3f9a-4e51-90f3-9e1d8077e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "======================================================================\n",
      "NEW MODEL INITIALIZED FOR 224x224 TRAINING\n",
      "======================================================================\n",
      "Previous model: 128x128 → 76.41% test AUC\n",
      "Target:         224x224 → 78-79% test AUC\n",
      "Expected gain:  +2-3% AUC\n",
      "Training time:  ~20-24 hours\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2575219465.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# INITIALIZE NEW MODEL FOR HIGH-RESOLUTION TRAINING\n",
    "# ========================================================================\n",
    "\n",
    "# Create fresh model (will train from ImageNet weights again)\n",
    "model_hires = ChestXrayClassifier(num_classes=14, pretrained=True)\n",
    "model_hires = model_hires.to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "optimizer = AdamW(model_hires.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 30\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "\n",
    "# New experiment\n",
    "experiment_name_hires = f\"chest_xray_224x224_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(f'experiments/{experiment_name_hires}', exist_ok=True)\n",
    "writer = SummaryWriter(f'experiments/{experiment_name_hires}/logs')\n",
    "\n",
    "config_hires = {\n",
    "    'model': 'EfficientNet-B0',\n",
    "    'image_size': IMAGE_SIZE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'previous_model': '128x128 (76.41% AUC)',\n",
    "    'target': '78-79% AUC',\n",
    "    'improvement_expected': '+2-3%'\n",
    "}\n",
    "\n",
    "with open(f'experiments/{experiment_name_hires}/config.json', 'w') as f:\n",
    "    json.dump(config_hires, f, indent=4)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NEW MODEL INITIALIZED FOR 224x224 TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Previous model: 128x128 → 76.41% test AUC\")\n",
    "print(f\"Target:         224x224 → 78-79% test AUC\")\n",
    "print(f\"Expected gain:  +2-3% AUC\")\n",
    "print(f\"Training time:  ~20-24 hours\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9a201a-be10-4306-a381-d9ff00419fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training functions ready\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ========================================================================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, epoch, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    print(f\"\\n🔄 Training Epoch {epoch+1}\")\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            predictions = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            all_predictions.append(predictions)\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            elapsed = time.time() - epoch_start\n",
    "            speed = (batch_idx + 1) / elapsed\n",
    "            eta = (len(loader) - batch_idx - 1) / speed / 60\n",
    "            print(f\"  [{batch_idx+1}/{len(loader)}] Loss: {avg_loss:.4f} | \"\n",
    "                  f\"{speed:.2f} batch/s | ETA: {eta:.1f}min\")\n",
    "    \n",
    "    if len(all_labels) > 0:\n",
    "        all_labels = np.vstack(all_labels)\n",
    "        all_predictions = np.vstack(all_predictions)\n",
    "        avg_loss = running_loss / len(loader)\n",
    "        try:\n",
    "            auc_macro = roc_auc_score(all_labels, all_predictions, average='macro')\n",
    "            auc_weighted = roc_auc_score(all_labels, all_predictions, average='weighted')\n",
    "        except:\n",
    "            auc_macro = 0.0\n",
    "            auc_weighted = 0.0\n",
    "    else:\n",
    "        avg_loss = running_loss / len(loader)\n",
    "        auc_macro = 0.0\n",
    "        auc_weighted = 0.0\n",
    "    \n",
    "    return avg_loss, auc_macro, auc_weighted\n",
    "\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_predictions.append(predictions)\n",
    "    \n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    \n",
    "    auc_macro = roc_auc_score(all_labels, all_predictions, average='macro')\n",
    "    auc_weighted = roc_auc_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    per_class_auc = []\n",
    "    for i in range(all_labels.shape[1]):\n",
    "        try:\n",
    "            auc = roc_auc_score(all_labels[:, i], all_predictions[:, i])\n",
    "            per_class_auc.append(auc)\n",
    "        except:\n",
    "            per_class_auc.append(0.0)\n",
    "    \n",
    "    return avg_loss, auc_macro, auc_weighted, per_class_auc\n",
    "\n",
    "print(\"✅ Training functions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca391ff0-1975-4b92-a60c-fe9f87813fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏰ Starting: 10:12 AM IST\n",
      "⏰ Expected: ~10:12 AM IST (tomorrow)\n",
      "\n",
      "======================================================================\n",
      "🚀 STARTING 224x224 HIGH-RESOLUTION TRAINING\n",
      "======================================================================\n",
      "Previous model (128x128): 76.41% test AUC\n",
      "Target (224x224):         78-79% test AUC\n",
      "Expected training time:   20-24 hours\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "EPOCH [1/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 1.3224 | 0.35 batch/s | ETA: 101.4min\n",
      "  [100/2164] Loss: 1.3472 | 0.35 batch/s | ETA: 99.1min\n",
      "  [150/2164] Loss: 1.3238 | 0.35 batch/s | ETA: 96.2min\n",
      "  [200/2164] Loss: 1.3261 | 0.35 batch/s | ETA: 93.7min\n",
      "  [250/2164] Loss: 1.3257 | 0.35 batch/s | ETA: 91.0min\n",
      "  [300/2164] Loss: 1.3332 | 0.35 batch/s | ETA: 88.7min\n",
      "  [350/2164] Loss: 1.3169 | 0.35 batch/s | ETA: 85.8min\n",
      "  [400/2164] Loss: 1.3117 | 0.35 batch/s | ETA: 84.0min\n",
      "  [450/2164] Loss: 1.3042 | 0.34 batch/s | ETA: 84.5min\n",
      "  [500/2164] Loss: 1.2902 | 0.34 batch/s | ETA: 80.5min\n",
      "  [550/2164] Loss: 1.2862 | 0.36 batch/s | ETA: 75.4min\n",
      "  [600/2164] Loss: 1.2688 | 0.37 batch/s | ETA: 70.8min\n",
      "  [650/2164] Loss: 1.2740 | 0.38 batch/s | ETA: 67.0min\n",
      "  [700/2164] Loss: 1.2699 | 0.39 batch/s | ETA: 63.3min\n",
      "  [750/2164] Loss: 1.2542 | 0.39 batch/s | ETA: 60.0min\n",
      "  [800/2164] Loss: 1.2461 | 0.40 batch/s | ETA: 56.9min\n",
      "  [850/2164] Loss: 1.2322 | 0.41 batch/s | ETA: 53.9min\n",
      "  [900/2164] Loss: 1.2318 | 0.41 batch/s | ETA: 51.1min\n",
      "  [950/2164] Loss: 1.2332 | 0.42 batch/s | ETA: 48.4min\n",
      "  [1000/2164] Loss: 1.2323 | 0.42 batch/s | ETA: 46.1min\n",
      "  [1050/2164] Loss: 1.2404 | 0.43 batch/s | ETA: 43.7min\n",
      "  [1100/2164] Loss: 1.2502 | 0.43 batch/s | ETA: 41.4min\n",
      "  [1150/2164] Loss: 1.2455 | 0.43 batch/s | ETA: 39.1min\n",
      "  [1200/2164] Loss: 1.2423 | 0.43 batch/s | ETA: 36.9min\n",
      "  [1250/2164] Loss: 1.2399 | 0.44 batch/s | ETA: 34.8min\n",
      "  [1300/2164] Loss: 1.2367 | 0.44 batch/s | ETA: 32.6min\n",
      "  [1350/2164] Loss: 1.2386 | 0.44 batch/s | ETA: 30.5min\n",
      "  [1400/2164] Loss: 1.2331 | 0.45 batch/s | ETA: 28.4min\n",
      "  [1450/2164] Loss: 1.2328 | 0.45 batch/s | ETA: 26.5min\n",
      "  [1500/2164] Loss: 1.2278 | 0.45 batch/s | ETA: 24.5min\n",
      "  [1550/2164] Loss: 1.2245 | 0.45 batch/s | ETA: 22.5min\n",
      "  [1600/2164] Loss: 1.2224 | 0.46 batch/s | ETA: 20.6min\n",
      "  [1650/2164] Loss: 1.2191 | 0.46 batch/s | ETA: 18.7min\n",
      "  [1700/2164] Loss: 1.2171 | 0.46 batch/s | ETA: 16.8min\n",
      "  [1750/2164] Loss: 1.2144 | 0.46 batch/s | ETA: 14.9min\n",
      "  [1800/2164] Loss: 1.2098 | 0.46 batch/s | ETA: 13.1min\n",
      "  [1850/2164] Loss: 1.2082 | 0.47 batch/s | ETA: 11.2min\n",
      "  [1900/2164] Loss: 1.2070 | 0.47 batch/s | ETA: 9.4min\n",
      "  [1950/2164] Loss: 1.2034 | 0.47 batch/s | ETA: 7.6min\n",
      "  [2000/2164] Loss: 1.2000 | 0.47 batch/s | ETA: 5.8min\n",
      "  [2050/2164] Loss: 1.1962 | 0.47 batch/s | ETA: 4.0min\n",
      "  [2100/2164] Loss: 1.1949 | 0.47 batch/s | ETA: 2.3min\n",
      "  [2150/2164] Loss: 1.1899 | 0.47 batch/s | ETA: 0.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results:\n",
      "  Train: Loss=1.1889, AUC=0.7433\n",
      "  Val:   Loss=1.1609, AUC=0.7956\n",
      "  Time: 92.0min | Total: 1.53hrs\n",
      "  vs 128x128: -0.0068 (-0.68%)\n",
      "\n",
      "  ✅ Best model saved! AUC: 0.7956\n",
      "\n",
      "======================================================================\n",
      "EPOCH [2/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 1.0430 | 0.55 batch/s | ETA: 64.6min\n",
      "  [100/2164] Loss: 0.9800 | 0.55 batch/s | ETA: 62.6min\n",
      "  [150/2164] Loss: 1.0067 | 0.55 batch/s | ETA: 61.2min\n",
      "  [200/2164] Loss: 1.1152 | 0.55 batch/s | ETA: 59.9min\n",
      "  [250/2164] Loss: 1.0763 | 0.54 batch/s | ETA: 58.8min\n",
      "  [300/2164] Loss: 1.0711 | 0.54 batch/s | ETA: 57.6min\n",
      "  [350/2164] Loss: 1.0859 | 0.54 batch/s | ETA: 56.0min\n",
      "  [400/2164] Loss: 1.0678 | 0.54 batch/s | ETA: 54.5min\n",
      "  [450/2164] Loss: 1.0830 | 0.54 batch/s | ETA: 53.0min\n",
      "  [500/2164] Loss: 1.0998 | 0.54 batch/s | ETA: 51.4min\n",
      "  [550/2164] Loss: 1.0903 | 0.54 batch/s | ETA: 49.9min\n",
      "  [600/2164] Loss: 1.0924 | 0.54 batch/s | ETA: 48.4min\n",
      "  [650/2164] Loss: 1.0840 | 0.54 batch/s | ETA: 46.9min\n",
      "  [700/2164] Loss: 1.0799 | 0.54 batch/s | ETA: 45.3min\n",
      "  [750/2164] Loss: 1.0704 | 0.54 batch/s | ETA: 43.7min\n",
      "  [800/2164] Loss: 1.0665 | 0.54 batch/s | ETA: 42.2min\n",
      "  [850/2164] Loss: 1.0652 | 0.54 batch/s | ETA: 40.7min\n",
      "  [900/2164] Loss: 1.0614 | 0.54 batch/s | ETA: 39.1min\n",
      "  [950/2164] Loss: 1.0652 | 0.54 batch/s | ETA: 37.6min\n",
      "  [1000/2164] Loss: 1.0566 | 0.54 batch/s | ETA: 36.0min\n",
      "  [1050/2164] Loss: 1.0564 | 0.54 batch/s | ETA: 34.5min\n",
      "  [1100/2164] Loss: 1.0607 | 0.54 batch/s | ETA: 33.0min\n",
      "  [1150/2164] Loss: 1.0585 | 0.54 batch/s | ETA: 31.4min\n",
      "  [1200/2164] Loss: 1.0571 | 0.54 batch/s | ETA: 29.9min\n",
      "  [1250/2164] Loss: 1.0506 | 0.54 batch/s | ETA: 28.4min\n",
      "  [1300/2164] Loss: 1.0538 | 0.54 batch/s | ETA: 26.8min\n",
      "  [1350/2164] Loss: 1.0587 | 0.54 batch/s | ETA: 25.3min\n",
      "  [1400/2164] Loss: 1.0560 | 0.53 batch/s | ETA: 23.8min\n",
      "  [1450/2164] Loss: 1.0628 | 0.53 batch/s | ETA: 22.3min\n",
      "  [1500/2164] Loss: 1.0597 | 0.53 batch/s | ETA: 20.7min\n",
      "  [1550/2164] Loss: 1.0589 | 0.54 batch/s | ETA: 19.1min\n",
      "  [1600/2164] Loss: 1.0645 | 0.53 batch/s | ETA: 17.6min\n",
      "  [1650/2164] Loss: 1.0666 | 0.53 batch/s | ETA: 16.0min\n",
      "  [1700/2164] Loss: 1.0619 | 0.53 batch/s | ETA: 14.5min\n",
      "  [1750/2164] Loss: 1.0742 | 0.53 batch/s | ETA: 12.9min\n",
      "  [1800/2164] Loss: 1.0722 | 0.53 batch/s | ETA: 11.3min\n",
      "  [1850/2164] Loss: 1.0732 | 0.53 batch/s | ETA: 9.8min\n",
      "  [1900/2164] Loss: 1.0715 | 0.53 batch/s | ETA: 8.2min\n",
      "  [1950/2164] Loss: 1.0684 | 0.53 batch/s | ETA: 6.7min\n",
      "  [2000/2164] Loss: 1.0679 | 0.53 batch/s | ETA: 5.1min\n",
      "  [2050/2164] Loss: 1.0661 | 0.53 batch/s | ETA: 3.6min\n",
      "  [2100/2164] Loss: 1.0631 | 0.54 batch/s | ETA: 2.0min\n",
      "  [2150/2164] Loss: 1.0609 | 0.54 batch/s | ETA: 0.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results:\n",
      "  Train: Loss=1.0604, AUC=0.8044\n",
      "  Val:   Loss=1.1231, AUC=0.8135\n",
      "  Time: 83.7min | Total: 2.93hrs\n",
      "  vs 128x128: +0.0111 (+1.11%)\n",
      "  ETA: 40.98 hours\n",
      "\n",
      "  ✅ Best model saved! AUC: 0.8135\n",
      "\n",
      "======================================================================\n",
      "EPOCH [3/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 0.9043 | 0.55 batch/s | ETA: 64.1min\n",
      "  [100/2164] Loss: 0.9662 | 0.54 batch/s | ETA: 63.3min\n",
      "  [150/2164] Loss: 0.9559 | 0.54 batch/s | ETA: 61.8min\n",
      "  [200/2164] Loss: 0.9475 | 0.54 batch/s | ETA: 60.4min\n",
      "  [250/2164] Loss: 0.9389 | 0.54 batch/s | ETA: 58.9min\n",
      "  [300/2164] Loss: 0.9354 | 0.54 batch/s | ETA: 57.7min\n",
      "  [350/2164] Loss: 0.9507 | 0.54 batch/s | ETA: 56.3min\n",
      "  [400/2164] Loss: 0.9542 | 0.54 batch/s | ETA: 54.8min\n",
      "  [450/2164] Loss: 0.9612 | 0.54 batch/s | ETA: 53.2min\n",
      "  [500/2164] Loss: 0.9572 | 0.54 batch/s | ETA: 51.7min\n",
      "  [550/2164] Loss: 0.9491 | 0.54 batch/s | ETA: 50.1min\n",
      "  [600/2164] Loss: 0.9453 | 0.54 batch/s | ETA: 48.5min\n",
      "  [650/2164] Loss: 0.9392 | 0.54 batch/s | ETA: 47.0min\n",
      "  [700/2164] Loss: 0.9434 | 0.54 batch/s | ETA: 45.5min\n",
      "  [750/2164] Loss: 0.9386 | 0.54 batch/s | ETA: 43.9min\n",
      "  [800/2164] Loss: 0.9390 | 0.54 batch/s | ETA: 42.4min\n",
      "  [850/2164] Loss: 0.9386 | 0.54 batch/s | ETA: 40.8min\n",
      "  [900/2164] Loss: 0.9406 | 0.54 batch/s | ETA: 39.3min\n",
      "  [950/2164] Loss: 0.9415 | 0.54 batch/s | ETA: 37.8min\n",
      "  [1000/2164] Loss: 0.9412 | 0.54 batch/s | ETA: 36.2min\n",
      "  [1050/2164] Loss: 0.9422 | 0.54 batch/s | ETA: 34.7min\n",
      "  [1100/2164] Loss: 0.9431 | 0.54 batch/s | ETA: 33.1min\n",
      "  [1150/2164] Loss: 0.9401 | 0.54 batch/s | ETA: 31.6min\n",
      "  [1200/2164] Loss: 0.9433 | 0.54 batch/s | ETA: 30.0min\n",
      "  [1250/2164] Loss: 0.9477 | 0.53 batch/s | ETA: 28.5min\n",
      "  [1300/2164] Loss: 0.9455 | 0.53 batch/s | ETA: 26.9min\n",
      "  [1350/2164] Loss: 0.9484 | 0.54 batch/s | ETA: 25.4min\n",
      "  [1400/2164] Loss: 0.9481 | 0.54 batch/s | ETA: 23.8min\n",
      "  [1450/2164] Loss: 0.9468 | 0.54 batch/s | ETA: 22.2min\n",
      "  [1500/2164] Loss: 0.9478 | 0.54 batch/s | ETA: 20.7min\n",
      "  [1550/2164] Loss: 0.9518 | 0.53 batch/s | ETA: 19.1min\n",
      "  [1600/2164] Loss: 0.9544 | 0.54 batch/s | ETA: 17.6min\n",
      "  [1650/2164] Loss: 0.9532 | 0.54 batch/s | ETA: 16.0min\n",
      "  [1700/2164] Loss: 0.9666 | 0.54 batch/s | ETA: 14.4min\n",
      "  [1750/2164] Loss: 0.9624 | 0.54 batch/s | ETA: 12.9min\n",
      "  [1800/2164] Loss: 0.9691 | 0.54 batch/s | ETA: 11.3min\n",
      "  [1850/2164] Loss: 0.9732 | 0.54 batch/s | ETA: 9.8min\n",
      "  [1900/2164] Loss: 0.9773 | 0.54 batch/s | ETA: 8.2min\n",
      "  [1950/2164] Loss: 0.9754 | 0.54 batch/s | ETA: 6.7min\n",
      "  [2000/2164] Loss: 0.9763 | 0.54 batch/s | ETA: 5.1min\n",
      "  [2050/2164] Loss: 0.9799 | 0.54 batch/s | ETA: 3.5min\n",
      "  [2100/2164] Loss: 0.9833 | 0.54 batch/s | ETA: 2.0min\n",
      "  [2150/2164] Loss: 0.9817 | 0.54 batch/s | ETA: 0.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results:\n",
      "  Train: Loss=0.9827, AUC=0.8408\n",
      "  Val:   Loss=1.0591, AUC=0.8238\n",
      "  Time: 83.5min | Total: 4.32hrs\n",
      "  vs 128x128: +0.0214 (+2.14%)\n",
      "  ETA: 38.87 hours\n",
      "\n",
      "  ✅ Best model saved! AUC: 0.8238\n",
      "\n",
      "======================================================================\n",
      "EPOCH [4/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 0.8516 | 0.54 batch/s | ETA: 65.4min\n",
      "  [100/2164] Loss: 0.8431 | 0.55 batch/s | ETA: 63.1min\n",
      "  [150/2164] Loss: 0.8827 | 0.54 batch/s | ETA: 61.8min\n",
      "  [200/2164] Loss: 0.8923 | 0.54 batch/s | ETA: 60.4min\n",
      "  [250/2164] Loss: 0.8997 | 0.54 batch/s | ETA: 59.5min\n",
      "  [300/2164] Loss: 0.9113 | 0.54 batch/s | ETA: 58.0min\n",
      "  [350/2164] Loss: 0.9126 | 0.53 batch/s | ETA: 56.7min\n",
      "  [400/2164] Loss: 0.9131 | 0.54 batch/s | ETA: 54.9min\n",
      "  [450/2164] Loss: 0.9067 | 0.54 batch/s | ETA: 53.3min\n",
      "  [500/2164] Loss: 0.9032 | 0.54 batch/s | ETA: 51.7min\n",
      "  [550/2164] Loss: 0.8947 | 0.53 batch/s | ETA: 50.7min\n",
      "  [600/2164] Loss: 0.8874 | 0.50 batch/s | ETA: 51.7min\n",
      "  [650/2164] Loss: 0.9025 | 0.48 batch/s | ETA: 52.7min\n",
      "  [700/2164] Loss: 0.8975 | 0.47 batch/s | ETA: 52.4min\n",
      "  [750/2164] Loss: 0.9075 | 0.45 batch/s | ETA: 52.0min\n",
      "  [800/2164] Loss: 0.9041 | 0.44 batch/s | ETA: 51.3min\n",
      "  [850/2164] Loss: 0.9002 | 0.43 batch/s | ETA: 50.6min\n",
      "  [900/2164] Loss: 0.8969 | 0.43 batch/s | ETA: 49.4min\n",
      "  [950/2164] Loss: 0.9020 | 0.42 batch/s | ETA: 48.0min\n",
      "  [1000/2164] Loss: 0.8978 | 0.42 batch/s | ETA: 46.6min\n",
      "  [1050/2164] Loss: 0.8974 | 0.40 batch/s | ETA: 45.9min\n",
      "  [1100/2164] Loss: 0.9013 | 0.37 batch/s | ETA: 48.3min\n",
      "  [1150/2164] Loss: 0.9065 | 0.37 batch/s | ETA: 45.5min\n",
      "  [1200/2164] Loss: 0.9041 | 0.37 batch/s | ETA: 43.0min\n",
      "  [1250/2164] Loss: 0.9011 | 0.38 batch/s | ETA: 40.5min\n",
      "  [1300/2164] Loss: 0.9042 | 0.38 batch/s | ETA: 38.1min\n",
      "  [1350/2164] Loss: 0.9017 | 0.38 batch/s | ETA: 35.7min\n",
      "  [1400/2164] Loss: 0.9010 | 0.38 batch/s | ETA: 33.3min\n",
      "  [1450/2164] Loss: 0.9059 | 0.38 batch/s | ETA: 31.0min\n",
      "  [1500/2164] Loss: 0.9064 | 0.39 batch/s | ETA: 28.7min\n",
      "  [1550/2164] Loss: 0.9091 | 0.39 batch/s | ETA: 26.4min\n",
      "  [1600/2164] Loss: 0.9103 | 0.39 batch/s | ETA: 24.2min\n",
      "  [1650/2164] Loss: 0.9140 | 0.39 batch/s | ETA: 22.0min\n",
      "  [1700/2164] Loss: 0.9153 | 0.39 batch/s | ETA: 19.8min\n",
      "  [1750/2164] Loss: 0.9126 | 0.39 batch/s | ETA: 17.6min\n",
      "  [1800/2164] Loss: 0.9107 | 0.39 batch/s | ETA: 15.5min\n",
      "  [1850/2164] Loss: 0.9155 | 0.39 batch/s | ETA: 13.3min\n",
      "  [1900/2164] Loss: 0.9163 | 0.39 batch/s | ETA: 11.2min\n",
      "  [1950/2164] Loss: 0.9153 | 0.40 batch/s | ETA: 9.0min\n",
      "  [2000/2164] Loss: 0.9154 | 0.40 batch/s | ETA: 6.9min\n",
      "  [2050/2164] Loss: 0.9191 | 0.40 batch/s | ETA: 4.8min\n",
      "  [2100/2164] Loss: 0.9168 | 0.40 batch/s | ETA: 2.7min\n",
      "  [2150/2164] Loss: 0.9167 | 0.40 batch/s | ETA: 0.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results:\n",
      "  Train: Loss=0.9172, AUC=0.8543\n",
      "  Val:   Loss=1.1489, AUC=0.8265\n",
      "  Time: 110.7min | Total: 6.16hrs\n",
      "  vs 128x128: +0.0241 (+2.41%)\n",
      "  ETA: 40.07 hours\n",
      "\n",
      "  ✅ Best model saved! AUC: 0.8265\n",
      "\n",
      "======================================================================\n",
      "EPOCH [5/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 0.7786 | 0.44 batch/s | ETA: 80.8min\n",
      "  [100/2164] Loss: 0.7972 | 0.43 batch/s | ETA: 80.3min\n",
      "  [150/2164] Loss: 0.7900 | 0.44 batch/s | ETA: 76.9min\n",
      "  [200/2164] Loss: 0.7871 | 0.43 batch/s | ETA: 75.7min\n",
      "  [250/2164] Loss: 0.8063 | 0.43 batch/s | ETA: 74.8min\n",
      "  [300/2164] Loss: 0.8055 | 0.43 batch/s | ETA: 72.8min\n",
      "  [350/2164] Loss: 0.8081 | 0.43 batch/s | ETA: 71.0min\n",
      "  [400/2164] Loss: 0.8081 | 0.43 batch/s | ETA: 68.5min\n",
      "  [450/2164] Loss: 0.8152 | 0.44 batch/s | ETA: 65.5min\n",
      "  [500/2164] Loss: 0.8052 | 0.44 batch/s | ETA: 63.2min\n",
      "  [550/2164] Loss: 0.8090 | 0.44 batch/s | ETA: 61.2min\n",
      "  [600/2164] Loss: 0.8073 | 0.45 batch/s | ETA: 58.4min\n",
      "  [650/2164] Loss: 0.8035 | 0.45 batch/s | ETA: 56.6min\n",
      "  [700/2164] Loss: 0.8055 | 0.45 batch/s | ETA: 54.6min\n",
      "  [750/2164] Loss: 0.8083 | 0.45 batch/s | ETA: 52.8min\n",
      "  [800/2164] Loss: 0.8085 | 0.44 batch/s | ETA: 51.1min\n",
      "  [850/2164] Loss: 0.8208 | 0.44 batch/s | ETA: 49.3min\n",
      "  [900/2164] Loss: 0.8174 | 0.44 batch/s | ETA: 47.4min\n",
      "  [950/2164] Loss: 0.8171 | 0.44 batch/s | ETA: 45.6min\n",
      "  [1000/2164] Loss: 0.8161 | 0.44 batch/s | ETA: 43.7min\n",
      "  [1050/2164] Loss: 0.8158 | 0.44 batch/s | ETA: 41.8min\n",
      "  [1100/2164] Loss: 0.8155 | 0.45 batch/s | ETA: 39.6min\n",
      "  [1150/2164] Loss: 0.8139 | 0.45 batch/s | ETA: 37.4min\n",
      "  [1200/2164] Loss: 0.8180 | 0.45 batch/s | ETA: 35.6min\n",
      "  [1250/2164] Loss: 0.8214 | 0.45 batch/s | ETA: 33.8min\n",
      "  [1300/2164] Loss: 0.8201 | 0.45 batch/s | ETA: 32.0min\n",
      "  [1350/2164] Loss: 0.8191 | 0.45 batch/s | ETA: 30.1min\n",
      "  [1400/2164] Loss: 0.8210 | 0.45 batch/s | ETA: 28.2min\n",
      "  [1450/2164] Loss: 0.8236 | 0.45 batch/s | ETA: 26.4min\n",
      "  [1500/2164] Loss: 0.8301 | 0.45 batch/s | ETA: 24.5min\n",
      "  [1550/2164] Loss: 0.8325 | 0.45 batch/s | ETA: 22.7min\n",
      "  [1600/2164] Loss: 0.8340 | 0.45 batch/s | ETA: 20.9min\n",
      "  [1650/2164] Loss: 0.8316 | 0.45 batch/s | ETA: 19.0min\n",
      "  [1700/2164] Loss: 0.8296 | 0.45 batch/s | ETA: 17.3min\n",
      "  [1750/2164] Loss: 0.8294 | 0.45 batch/s | ETA: 15.5min\n",
      "  [1800/2164] Loss: 0.8282 | 0.44 batch/s | ETA: 13.7min\n",
      "  [1850/2164] Loss: 0.8320 | 0.44 batch/s | ETA: 11.8min\n",
      "  [1900/2164] Loss: 0.8314 | 0.44 batch/s | ETA: 10.0min\n",
      "  [1950/2164] Loss: 0.8308 | 0.44 batch/s | ETA: 8.1min\n",
      "  [2000/2164] Loss: 0.8302 | 0.43 batch/s | ETA: 6.4min\n",
      "  [2050/2164] Loss: 0.8310 | 0.43 batch/s | ETA: 4.4min\n",
      "  [2100/2164] Loss: 0.8318 | 0.43 batch/s | ETA: 2.5min\n",
      "  [2150/2164] Loss: 0.8325 | 0.43 batch/s | ETA: 0.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results:\n",
      "  Train: Loss=0.8324, AUC=0.8680\n",
      "  Val:   Loss=1.1965, AUC=0.8224\n",
      "  Time: 103.1min | Total: 7.88hrs\n",
      "  vs 128x128: +0.0200 (+2.00%)\n",
      "  ETA: 39.41 hours\n",
      "\n",
      "  ⏳ No improvement (1/7)\n",
      "\n",
      "======================================================================\n",
      "EPOCH [6/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 0.7006 | 0.45 batch/s | ETA: 77.9min\n",
      "  [100/2164] Loss: 0.7356 | 0.46 batch/s | ETA: 74.8min\n",
      "  [150/2164] Loss: 0.7376 | 0.46 batch/s | ETA: 73.2min\n",
      "  [200/2164] Loss: 0.7700 | 0.46 batch/s | ETA: 71.7min\n",
      "  [250/2164] Loss: 0.7620 | 0.45 batch/s | ETA: 70.2min\n",
      "  [300/2164] Loss: 0.7707 | 0.45 batch/s | ETA: 68.5min\n",
      "  [350/2164] Loss: 0.7750 | 0.45 batch/s | ETA: 67.2min\n",
      "  [400/2164] Loss: 0.7688 | 0.45 batch/s | ETA: 65.9min\n",
      "  [450/2164] Loss: 0.7800 | 0.44 batch/s | ETA: 64.3min\n",
      "  [500/2164] Loss: 0.7751 | 0.44 batch/s | ETA: 62.6min\n",
      "  [550/2164] Loss: 0.7777 | 0.44 batch/s | ETA: 60.7min\n",
      "  [600/2164] Loss: 0.7676 | 0.44 batch/s | ETA: 58.8min\n",
      "  [650/2164] Loss: 0.7719 | 0.44 batch/s | ETA: 57.0min\n",
      "  [700/2164] Loss: 0.7840 | 0.44 batch/s | ETA: 55.2min\n",
      "  [750/2164] Loss: 0.7797 | 0.44 batch/s | ETA: 53.4min\n",
      "  [800/2164] Loss: 0.7775 | 0.44 batch/s | ETA: 51.5min\n",
      "  [850/2164] Loss: 0.7759 | 0.44 batch/s | ETA: 49.6min\n",
      "  [900/2164] Loss: 0.7749 | 0.44 batch/s | ETA: 47.8min\n",
      "  [950/2164] Loss: 0.7742 | 0.44 batch/s | ETA: 45.9min\n",
      "  [1000/2164] Loss: 0.7686 | 0.44 batch/s | ETA: 44.2min\n",
      "  [1050/2164] Loss: 0.7672 | 0.43 batch/s | ETA: 42.8min\n",
      "  [1100/2164] Loss: 0.7678 | 0.43 batch/s | ETA: 41.3min\n",
      "  [1150/2164] Loss: 0.7715 | 0.43 batch/s | ETA: 39.7min\n",
      "  [1200/2164] Loss: 0.7693 | 0.43 batch/s | ETA: 37.8min\n",
      "  [1250/2164] Loss: 0.7681 | 0.42 batch/s | ETA: 35.8min\n",
      "  [1300/2164] Loss: 0.7657 | 0.43 batch/s | ETA: 33.8min\n",
      "  [1350/2164] Loss: 0.7665 | 0.43 batch/s | ETA: 31.5min\n",
      "  [1400/2164] Loss: 0.7657 | 0.43 batch/s | ETA: 29.3min\n",
      "  [1450/2164] Loss: 0.7641 | 0.44 batch/s | ETA: 27.2min\n",
      "  [1500/2164] Loss: 0.7654 | 0.44 batch/s | ETA: 25.1min\n",
      "  [1550/2164] Loss: 0.7648 | 0.45 batch/s | ETA: 23.0min\n",
      "  [1600/2164] Loss: 0.7664 | 0.45 batch/s | ETA: 21.0min\n",
      "  [1650/2164] Loss: 0.7657 | 0.45 batch/s | ETA: 19.0min\n",
      "  [1700/2164] Loss: 0.7649 | 0.45 batch/s | ETA: 17.0min\n",
      "  [1750/2164] Loss: 0.7664 | 0.46 batch/s | ETA: 15.1min\n",
      "  [1800/2164] Loss: 0.7650 | 0.46 batch/s | ETA: 13.2min\n",
      "  [1850/2164] Loss: 0.7653 | 0.46 batch/s | ETA: 11.3min\n",
      "  [1900/2164] Loss: 0.7642 | 0.46 batch/s | ETA: 9.5min\n",
      "  [1950/2164] Loss: 0.7632 | 0.47 batch/s | ETA: 7.6min\n",
      "  [2000/2164] Loss: 0.7635 | 0.47 batch/s | ETA: 5.8min\n",
      "  [2050/2164] Loss: 0.7625 | 0.47 batch/s | ETA: 4.0min\n",
      "  [2100/2164] Loss: 0.7621 | 0.47 batch/s | ETA: 2.3min\n",
      "  [2150/2164] Loss: 0.7613 | 0.47 batch/s | ETA: 0.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results:\n",
      "  Train: Loss=0.7648, AUC=0.8835\n",
      "  Val:   Loss=1.3413, AUC=0.8209\n",
      "  Time: 92.6min | Total: 9.43hrs\n",
      "  vs 128x128: +0.0185 (+1.85%)\n",
      "  ETA: 37.70 hours\n",
      "\n",
      "  ⏳ No improvement (2/7)\n",
      "\n",
      "======================================================================\n",
      "EPOCH [7/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 0.6789 | 0.74 batch/s | ETA: 47.9min\n",
      "  [100/2164] Loss: 0.6713 | 0.73 batch/s | ETA: 47.3min\n",
      "  [150/2164] Loss: 0.7005 | 0.72 batch/s | ETA: 46.5min\n",
      "  [200/2164] Loss: 0.6886 | 0.72 batch/s | ETA: 45.5min\n",
      "  [250/2164] Loss: 0.7091 | 0.71 batch/s | ETA: 45.1min\n",
      "  [300/2164] Loss: 0.7257 | 0.70 batch/s | ETA: 44.4min\n",
      "  [350/2164] Loss: 0.7256 | 0.70 batch/s | ETA: 43.4min\n",
      "  [400/2164] Loss: 0.7192 | 0.69 batch/s | ETA: 42.3min\n",
      "  [450/2164] Loss: 0.7163 | 0.69 batch/s | ETA: 41.3min\n",
      "  [500/2164] Loss: 0.7181 | 0.69 batch/s | ETA: 40.1min\n",
      "  [550/2164] Loss: 0.7119 | 0.69 batch/s | ETA: 38.9min\n",
      "  [600/2164] Loss: 0.7121 | 0.69 batch/s | ETA: 37.5min\n",
      "  [650/2164] Loss: 0.7106 | 0.70 batch/s | ETA: 36.3min\n",
      "  [700/2164] Loss: 0.7105 | 0.70 batch/s | ETA: 35.1min\n",
      "  [750/2164] Loss: 0.7050 | 0.70 batch/s | ETA: 33.8min\n",
      "  [800/2164] Loss: 0.7024 | 0.69 batch/s | ETA: 33.0min\n",
      "  [850/2164] Loss: 0.7019 | 0.68 batch/s | ETA: 32.3min\n",
      "  [900/2164] Loss: 0.7003 | 0.66 batch/s | ETA: 31.7min\n",
      "  [950/2164] Loss: 0.7006 | 0.66 batch/s | ETA: 30.6min\n",
      "  [1000/2164] Loss: 0.7002 | 0.66 batch/s | ETA: 29.6min\n",
      "  [1050/2164] Loss: 0.7005 | 0.65 batch/s | ETA: 28.5min\n",
      "  [1100/2164] Loss: 0.6995 | 0.65 batch/s | ETA: 27.5min\n",
      "  [1150/2164] Loss: 0.6986 | 0.64 batch/s | ETA: 26.4min\n",
      "  [1200/2164] Loss: 0.7058 | 0.64 batch/s | ETA: 25.2min\n",
      "  [1250/2164] Loss: 0.7065 | 0.64 batch/s | ETA: 23.9min\n",
      "  [1300/2164] Loss: 0.7061 | 0.63 batch/s | ETA: 22.8min\n",
      "  [1350/2164] Loss: 0.7039 | 0.63 batch/s | ETA: 21.5min\n",
      "  [1400/2164] Loss: 0.7019 | 0.63 batch/s | ETA: 20.2min\n",
      "  [1450/2164] Loss: 0.7028 | 0.63 batch/s | ETA: 18.9min\n",
      "  [1500/2164] Loss: 0.7019 | 0.63 batch/s | ETA: 17.6min\n",
      "  [1550/2164] Loss: 0.7014 | 0.63 batch/s | ETA: 16.3min\n",
      "  [1600/2164] Loss: 0.7014 | 0.63 batch/s | ETA: 14.9min\n",
      "  [1650/2164] Loss: 0.7008 | 0.63 batch/s | ETA: 13.6min\n",
      "  [1700/2164] Loss: 0.7016 | 0.63 batch/s | ETA: 12.3min\n",
      "  [1750/2164] Loss: 0.7010 | 0.63 batch/s | ETA: 10.9min\n",
      "  [1800/2164] Loss: 0.7010 | 0.63 batch/s | ETA: 9.6min\n",
      "  [1850/2164] Loss: 0.7006 | 0.63 batch/s | ETA: 8.3min\n",
      "  [1900/2164] Loss: 0.7017 | 0.63 batch/s | ETA: 6.9min\n",
      "  [1950/2164] Loss: 0.7019 | 0.64 batch/s | ETA: 5.6min\n",
      "  [2000/2164] Loss: 0.7019 | 0.64 batch/s | ETA: 4.3min\n",
      "  [2050/2164] Loss: 0.7042 | 0.64 batch/s | ETA: 3.0min\n",
      "  [2100/2164] Loss: 0.7039 | 0.64 batch/s | ETA: 1.7min\n",
      "  [2150/2164] Loss: 0.7038 | 0.64 batch/s | ETA: 0.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results:\n",
      "  Train: Loss=0.7048, AUC=0.8975\n",
      "  Val:   Loss=1.5830, AUC=0.8184\n",
      "  Time: 69.8min | Total: 10.59hrs\n",
      "  vs 128x128: +0.0160 (+1.60%)\n",
      "  ETA: 34.79 hours\n",
      "\n",
      "  ⏳ No improvement (3/7)\n",
      "\n",
      "======================================================================\n",
      "EPOCH [8/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 0.6224 | 0.68 batch/s | ETA: 52.0min\n",
      "  [100/2164] Loss: 0.6089 | 0.67 batch/s | ETA: 51.0min\n",
      "  [150/2164] Loss: 0.6083 | 0.68 batch/s | ETA: 49.4min\n",
      "  [200/2164] Loss: 0.6202 | 0.68 batch/s | ETA: 48.1min\n",
      "  [250/2164] Loss: 0.6134 | 0.68 batch/s | ETA: 47.0min\n",
      "  [300/2164] Loss: 0.6186 | 0.68 batch/s | ETA: 45.9min\n",
      "  [350/2164] Loss: 0.6175 | 0.68 batch/s | ETA: 44.8min\n",
      "  [400/2164] Loss: 0.6118 | 0.67 batch/s | ETA: 43.6min\n",
      "  [450/2164] Loss: 0.6173 | 0.67 batch/s | ETA: 42.4min\n",
      "  [500/2164] Loss: 0.6192 | 0.67 batch/s | ETA: 41.2min\n",
      "  [550/2164] Loss: 0.6201 | 0.67 batch/s | ETA: 40.2min\n",
      "  [600/2164] Loss: 0.6235 | 0.67 batch/s | ETA: 38.8min\n",
      "  [650/2164] Loss: 0.6367 | 0.67 batch/s | ETA: 37.6min\n",
      "  [700/2164] Loss: 0.6427 | 0.67 batch/s | ETA: 36.4min\n",
      "  [750/2164] Loss: 0.6433 | 0.67 batch/s | ETA: 35.2min\n",
      "  [800/2164] Loss: 0.6457 | 0.67 batch/s | ETA: 33.9min\n",
      "  [850/2164] Loss: 0.6478 | 0.67 batch/s | ETA: 32.7min\n",
      "  [900/2164] Loss: 0.6471 | 0.67 batch/s | ETA: 31.4min\n",
      "  [950/2164] Loss: 0.6462 | 0.67 batch/s | ETA: 30.2min\n",
      "  [1000/2164] Loss: 0.6489 | 0.67 batch/s | ETA: 28.9min\n",
      "  [1050/2164] Loss: 0.6485 | 0.67 batch/s | ETA: 27.7min\n",
      "  [1100/2164] Loss: 0.6484 | 0.67 batch/s | ETA: 26.4min\n",
      "  [1150/2164] Loss: 0.6459 | 0.67 batch/s | ETA: 25.2min\n",
      "  [1200/2164] Loss: 0.6455 | 0.67 batch/s | ETA: 23.9min\n",
      "  [1250/2164] Loss: 0.6464 | 0.66 batch/s | ETA: 23.0min\n",
      "  [1300/2164] Loss: 0.6509 | 0.65 batch/s | ETA: 22.3min\n",
      "  [1350/2164] Loss: 0.6502 | 0.64 batch/s | ETA: 21.3min\n",
      "  [1400/2164] Loss: 0.6498 | 0.63 batch/s | ETA: 20.3min\n",
      "  [1450/2164] Loss: 0.6503 | 0.62 batch/s | ETA: 19.2min\n",
      "  [1500/2164] Loss: 0.6492 | 0.61 batch/s | ETA: 18.2min\n",
      "  [1550/2164] Loss: 0.6534 | 0.59 batch/s | ETA: 17.4min\n",
      "  [1600/2164] Loss: 0.6525 | 0.58 batch/s | ETA: 16.3min\n",
      "  [1650/2164] Loss: 0.6533 | 0.57 batch/s | ETA: 15.1min\n",
      "  [1700/2164] Loss: 0.6530 | 0.56 batch/s | ETA: 13.9min\n",
      "  [1750/2164] Loss: 0.6571 | 0.54 batch/s | ETA: 12.7min\n",
      "  [1800/2164] Loss: 0.6565 | 0.53 batch/s | ETA: 11.4min\n",
      "  [1850/2164] Loss: 0.6560 | 0.52 batch/s | ETA: 10.0min\n",
      "  [1900/2164] Loss: 0.6558 | 0.51 batch/s | ETA: 8.6min\n",
      "  [1950/2164] Loss: 0.6547 | 0.51 batch/s | ETA: 7.0min\n",
      "  [2000/2164] Loss: 0.6549 | 0.51 batch/s | ETA: 5.3min\n",
      "  [2050/2164] Loss: 0.6576 | 0.52 batch/s | ETA: 3.7min\n",
      "  [2100/2164] Loss: 0.6575 | 0.52 batch/s | ETA: 2.1min\n",
      "  [2150/2164] Loss: 0.6603 | 0.52 batch/s | ETA: 0.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📉 LR: 1.00e-04 → 5.00e-05\n",
      "\n",
      "📊 Results:\n",
      "  Train: Loss=0.6596, AUC=0.9074\n",
      "  Val:   Loss=1.8609, AUC=0.8124\n",
      "  Time: 81.4min | Total: 11.95hrs\n",
      "  vs 128x128: +0.0100 (+1.00%)\n",
      "  ETA: 32.85 hours\n",
      "\n",
      "  ⏳ No improvement (4/7)\n",
      "\n",
      "======================================================================\n",
      "EPOCH [9/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 0.5522 | 0.75 batch/s | ETA: 47.2min\n",
      "  [100/2164] Loss: 0.5587 | 0.75 batch/s | ETA: 46.0min\n",
      "  [150/2164] Loss: 0.5796 | 0.74 batch/s | ETA: 45.3min\n",
      "  [200/2164] Loss: 0.5746 | 0.74 batch/s | ETA: 44.5min\n",
      "  [250/2164] Loss: 0.5762 | 0.73 batch/s | ETA: 43.5min\n",
      "  [300/2164] Loss: 0.5766 | 0.73 batch/s | ETA: 42.6min\n",
      "  [350/2164] Loss: 0.5882 | 0.72 batch/s | ETA: 41.7min\n",
      "  [400/2164] Loss: 0.5836 | 0.72 batch/s | ETA: 40.9min\n",
      "  [450/2164] Loss: 0.5789 | 0.72 batch/s | ETA: 39.9min\n",
      "  [500/2164] Loss: 0.5807 | 0.71 batch/s | ETA: 38.9min\n",
      "  [550/2164] Loss: 0.5815 | 0.71 batch/s | ETA: 37.8min\n",
      "  [600/2164] Loss: 0.5778 | 0.71 batch/s | ETA: 36.6min\n",
      "  [650/2164] Loss: 0.5776 | 0.71 batch/s | ETA: 35.5min\n",
      "  [700/2164] Loss: 0.5791 | 0.71 batch/s | ETA: 34.5min\n",
      "  [750/2164] Loss: 0.5791 | 0.71 batch/s | ETA: 33.3min\n",
      "  [800/2164] Loss: 0.5776 | 0.71 batch/s | ETA: 32.1min\n",
      "  [850/2164] Loss: 0.5761 | 0.71 batch/s | ETA: 31.0min\n",
      "  [900/2164] Loss: 0.5748 | 0.71 batch/s | ETA: 29.8min\n",
      "  [950/2164] Loss: 0.5765 | 0.71 batch/s | ETA: 28.7min\n",
      "  [1000/2164] Loss: 0.5746 | 0.71 batch/s | ETA: 27.5min\n",
      "  [1050/2164] Loss: 0.5767 | 0.70 batch/s | ETA: 26.4min\n",
      "  [1100/2164] Loss: 0.5766 | 0.70 batch/s | ETA: 25.2min\n",
      "  [1150/2164] Loss: 0.5789 | 0.70 batch/s | ETA: 24.0min\n",
      "  [1200/2164] Loss: 0.5792 | 0.70 batch/s | ETA: 22.8min\n",
      "  [1250/2164] Loss: 0.5787 | 0.70 batch/s | ETA: 21.7min\n",
      "  [1300/2164] Loss: 0.5791 | 0.70 batch/s | ETA: 20.5min\n",
      "  [1350/2164] Loss: 0.5787 | 0.70 batch/s | ETA: 19.3min\n",
      "  [1400/2164] Loss: 0.5780 | 0.71 batch/s | ETA: 18.1min\n",
      "  [1450/2164] Loss: 0.5762 | 0.71 batch/s | ETA: 16.9min\n",
      "  [1500/2164] Loss: 0.5780 | 0.71 batch/s | ETA: 15.7min\n",
      "  [1550/2164] Loss: 0.5793 | 0.71 batch/s | ETA: 14.5min\n",
      "  [1600/2164] Loss: 0.5785 | 0.71 batch/s | ETA: 13.3min\n",
      "  [1650/2164] Loss: 0.5777 | 0.71 batch/s | ETA: 12.1min\n",
      "  [1700/2164] Loss: 0.5822 | 0.71 batch/s | ETA: 11.0min\n",
      "  [1750/2164] Loss: 0.5809 | 0.71 batch/s | ETA: 9.8min\n",
      "  [1800/2164] Loss: 0.5800 | 0.71 batch/s | ETA: 8.6min\n",
      "  [1850/2164] Loss: 0.5784 | 0.71 batch/s | ETA: 7.4min\n",
      "  [1900/2164] Loss: 0.5791 | 0.71 batch/s | ETA: 6.2min\n",
      "  [1950/2164] Loss: 0.5794 | 0.71 batch/s | ETA: 5.0min\n",
      "  [2000/2164] Loss: 0.5788 | 0.71 batch/s | ETA: 3.9min\n",
      "  [2050/2164] Loss: 0.5784 | 0.71 batch/s | ETA: 2.7min\n",
      "  [2100/2164] Loss: 0.5788 | 0.71 batch/s | ETA: 1.5min\n",
      "  [2150/2164] Loss: 0.5771 | 0.71 batch/s | ETA: 0.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results:\n",
      "  Train: Loss=0.5778, AUC=0.9221\n",
      "  Val:   Loss=2.0247, AUC=0.8089\n",
      "  Time: 63.6min | Total: 13.01hrs\n",
      "  vs 128x128: +0.0065 (+0.65%)\n",
      "  ETA: 30.35 hours\n",
      "\n",
      "  ⏳ No improvement (5/7)\n",
      "\n",
      "======================================================================\n",
      "EPOCH [10/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 0.4979 | 0.73 batch/s | ETA: 48.5min\n",
      "  [100/2164] Loss: 0.5279 | 0.72 batch/s | ETA: 47.5min\n",
      "  [150/2164] Loss: 0.5412 | 0.73 batch/s | ETA: 45.9min\n",
      "  [200/2164] Loss: 0.5354 | 0.73 batch/s | ETA: 44.9min\n",
      "  [250/2164] Loss: 0.5294 | 0.72 batch/s | ETA: 44.1min\n",
      "  [300/2164] Loss: 0.5247 | 0.72 batch/s | ETA: 43.0min\n",
      "  [350/2164] Loss: 0.5285 | 0.72 batch/s | ETA: 41.8min\n",
      "  [400/2164] Loss: 0.5272 | 0.72 batch/s | ETA: 40.8min\n",
      "  [450/2164] Loss: 0.5221 | 0.72 batch/s | ETA: 39.6min\n",
      "  [500/2164] Loss: 0.5201 | 0.72 batch/s | ETA: 38.4min\n",
      "  [550/2164] Loss: 0.5182 | 0.72 batch/s | ETA: 37.4min\n",
      "  [600/2164] Loss: 0.5186 | 0.72 batch/s | ETA: 36.3min\n",
      "  [650/2164] Loss: 0.5172 | 0.72 batch/s | ETA: 35.1min\n",
      "  [700/2164] Loss: 0.5179 | 0.72 batch/s | ETA: 34.0min\n",
      "  [750/2164] Loss: 0.5174 | 0.72 batch/s | ETA: 32.8min\n",
      "  [800/2164] Loss: 0.5179 | 0.72 batch/s | ETA: 31.7min\n",
      "  [850/2164] Loss: 0.5191 | 0.72 batch/s | ETA: 30.5min\n",
      "  [900/2164] Loss: 0.5188 | 0.72 batch/s | ETA: 29.4min\n",
      "  [950/2164] Loss: 0.5198 | 0.72 batch/s | ETA: 28.2min\n",
      "  [1000/2164] Loss: 0.5200 | 0.72 batch/s | ETA: 27.0min\n",
      "  [1050/2164] Loss: 0.5195 | 0.72 batch/s | ETA: 25.9min\n",
      "  [1100/2164] Loss: 0.5200 | 0.72 batch/s | ETA: 24.8min\n",
      "  [1150/2164] Loss: 0.5211 | 0.72 batch/s | ETA: 23.6min\n",
      "  [1200/2164] Loss: 0.5203 | 0.71 batch/s | ETA: 22.5min\n",
      "  [1250/2164] Loss: 0.5205 | 0.71 batch/s | ETA: 21.4min\n",
      "  [1300/2164] Loss: 0.5209 | 0.71 batch/s | ETA: 20.2min\n",
      "  [1350/2164] Loss: 0.5221 | 0.71 batch/s | ETA: 19.1min\n",
      "  [1400/2164] Loss: 0.5237 | 0.71 batch/s | ETA: 17.9min\n",
      "  [1450/2164] Loss: 0.5249 | 0.71 batch/s | ETA: 16.8min\n",
      "  [1500/2164] Loss: 0.5261 | 0.71 batch/s | ETA: 15.6min\n",
      "  [1550/2164] Loss: 0.5252 | 0.71 batch/s | ETA: 14.4min\n",
      "  [1600/2164] Loss: 0.5249 | 0.71 batch/s | ETA: 13.2min\n",
      "  [1650/2164] Loss: 0.5310 | 0.71 batch/s | ETA: 12.1min\n",
      "  [1700/2164] Loss: 0.5316 | 0.71 batch/s | ETA: 10.9min\n",
      "  [1750/2164] Loss: 0.5320 | 0.71 batch/s | ETA: 9.7min\n",
      "  [1800/2164] Loss: 0.5305 | 0.71 batch/s | ETA: 8.5min\n",
      "  [1850/2164] Loss: 0.5343 | 0.71 batch/s | ETA: 7.4min\n",
      "  [1900/2164] Loss: 0.5361 | 0.71 batch/s | ETA: 6.2min\n",
      "  [1950/2164] Loss: 0.5359 | 0.71 batch/s | ETA: 5.0min\n",
      "  [2000/2164] Loss: 0.5358 | 0.71 batch/s | ETA: 3.9min\n",
      "  [2050/2164] Loss: 0.5361 | 0.71 batch/s | ETA: 2.7min\n",
      "  [2100/2164] Loss: 0.5367 | 0.71 batch/s | ETA: 1.5min\n",
      "  [2150/2164] Loss: 0.5403 | 0.71 batch/s | ETA: 0.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results:\n",
      "  Train: Loss=0.5398, AUC=0.9291\n",
      "  Val:   Loss=2.1835, AUC=0.8070\n",
      "  Time: 65.0min | Total: 14.09hrs\n",
      "  vs 128x128: +0.0046 (+0.46%)\n",
      "  ETA: 28.18 hours\n",
      "\n",
      "  ⏳ No improvement (6/7)\n",
      "\n",
      "======================================================================\n",
      "EPOCH [11/30]\n",
      "======================================================================\n",
      "\n",
      "🔄 Training Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [50/2164] Loss: 0.4736 | 0.48 batch/s | ETA: 72.9min\n",
      "  [100/2164] Loss: 0.4768 | 0.47 batch/s | ETA: 72.5min\n",
      "  [150/2164] Loss: 0.4812 | 0.47 batch/s | ETA: 71.0min\n",
      "  [200/2164] Loss: 0.4845 | 0.47 batch/s | ETA: 69.6min\n",
      "  [250/2164] Loss: 0.4876 | 0.47 batch/s | ETA: 68.2min\n",
      "  [300/2164] Loss: 0.4974 | 0.46 batch/s | ETA: 67.5min\n",
      "  [350/2164] Loss: 0.4922 | 0.45 batch/s | ETA: 67.1min\n",
      "  [400/2164] Loss: 0.4972 | 0.44 batch/s | ETA: 66.1min\n",
      "  [450/2164] Loss: 0.4986 | 0.44 batch/s | ETA: 64.8min\n",
      "  [500/2164] Loss: 0.4983 | 0.44 batch/s | ETA: 62.6min\n",
      "  [550/2164] Loss: 0.4955 | 0.44 batch/s | ETA: 61.3min\n",
      "  [600/2164] Loss: 0.4967 | 0.44 batch/s | ETA: 59.6min\n",
      "  [650/2164] Loss: 0.4960 | 0.45 batch/s | ETA: 56.2min\n",
      "  [700/2164] Loss: 0.4972 | 0.46 batch/s | ETA: 53.0min\n",
      "  [750/2164] Loss: 0.4986 | 0.47 batch/s | ETA: 50.0min\n",
      "  [800/2164] Loss: 0.4969 | 0.48 batch/s | ETA: 47.2min\n",
      "  [850/2164] Loss: 0.4999 | 0.49 batch/s | ETA: 44.6min\n",
      "  [900/2164] Loss: 0.4984 | 0.50 batch/s | ETA: 42.2min\n",
      "  [950/2164] Loss: 0.4992 | 0.51 batch/s | ETA: 39.9min\n",
      "  [1000/2164] Loss: 0.5002 | 0.51 batch/s | ETA: 37.7min\n",
      "  [1050/2164] Loss: 0.4999 | 0.52 batch/s | ETA: 35.6min\n",
      "  [1100/2164] Loss: 0.5060 | 0.53 batch/s | ETA: 33.6min\n",
      "  [1150/2164] Loss: 0.5054 | 0.53 batch/s | ETA: 31.7min\n",
      "  [1200/2164] Loss: 0.5059 | 0.54 batch/s | ETA: 29.9min\n",
      "  [1250/2164] Loss: 0.5058 | 0.54 batch/s | ETA: 28.0min\n",
      "  [1300/2164] Loss: 0.5046 | 0.55 batch/s | ETA: 26.3min\n",
      "  [1350/2164] Loss: 0.5048 | 0.55 batch/s | ETA: 24.5min\n",
      "  [1400/2164] Loss: 0.5034 | 0.56 batch/s | ETA: 22.8min\n",
      "  [1450/2164] Loss: 0.5045 | 0.56 batch/s | ETA: 21.2min\n",
      "  [1500/2164] Loss: 0.5055 | 0.57 batch/s | ETA: 19.6min\n",
      "  [1550/2164] Loss: 0.5064 | 0.57 batch/s | ETA: 18.0min\n",
      "  [1600/2164] Loss: 0.5070 | 0.57 batch/s | ETA: 16.4min\n",
      "  [1650/2164] Loss: 0.5072 | 0.58 batch/s | ETA: 14.9min\n",
      "  [1700/2164] Loss: 0.5070 | 0.58 batch/s | ETA: 13.3min\n",
      "  [1750/2164] Loss: 0.5069 | 0.58 batch/s | ETA: 11.8min\n",
      "  [1800/2164] Loss: 0.5066 | 0.59 batch/s | ETA: 10.4min\n",
      "  [1850/2164] Loss: 0.5072 | 0.59 batch/s | ETA: 8.9min\n",
      "  [1900/2164] Loss: 0.5073 | 0.59 batch/s | ETA: 7.4min\n",
      "  [1950/2164] Loss: 0.5094 | 0.59 batch/s | ETA: 6.0min\n",
      "  [2000/2164] Loss: 0.5090 | 0.60 batch/s | ETA: 4.6min\n",
      "  [2050/2164] Loss: 0.5095 | 0.60 batch/s | ETA: 3.2min\n",
      "  [2100/2164] Loss: 0.5089 | 0.60 batch/s | ETA: 1.8min\n",
      "  [2150/2164] Loss: 0.5093 | 0.60 batch/s | ETA: 0.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_20112\\2896649790.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results:\n",
      "  Train: Loss=0.5089, AUC=0.9337\n",
      "  Val:   Loss=2.4142, AUC=0.8089\n",
      "  Time: 72.4min | Total: 15.30hrs\n",
      "  vs 128x128: +0.0065 (+0.65%)\n",
      "  ETA: 26.42 hours\n",
      "\n",
      "  ⏳ No improvement (7/7)\n",
      "\n",
      "⚠️ Early stopping at epoch 11\n",
      "\n",
      "======================================================================\n",
      "✅ 224x224 TRAINING COMPLETE!\n",
      "Best AUC: 0.8265\n",
      "Time: 15.30 hours\n",
      "Improvement over 128x128: +2.41%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# START 224x224 HIGH-RESOLUTION TRAINING\n",
    "# ========================================================================\n",
    "\n",
    "def train_model_hires():\n",
    "    best_val_auc = 0.0\n",
    "    early_stop_counter = 0\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    pathologies = [\n",
    "        'Atelectasis', 'Consolidation', 'Infiltration', \n",
    "        'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', \n",
    "        'Effusion', 'Pneumonia', 'Pleural_Thickening', \n",
    "        'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🚀 STARTING 224x224 HIGH-RESOLUTION TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Previous model (128x128): 76.41% test AUC\")\n",
    "    print(f\"Target (224x224):         78-79% test AUC\")\n",
    "    print(f\"Expected training time:   20-24 hours\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EPOCH [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_auc_macro, train_auc_weighted = train_epoch(\n",
    "            model_hires, train_loader, criterion, optimizer, device, epoch, scaler\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_auc_macro, val_auc_weighted, per_class_auc = validate_epoch(\n",
    "            model_hires, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Update LR\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_auc_macro)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if old_lr != current_lr:\n",
    "            print(f\"\\n📉 LR: {old_lr:.2e} → {current_lr:.2e}\")\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        total_elapsed = (time.time() - training_start_time) / 3600\n",
    "        \n",
    "        # Log\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('AUC/train', train_auc_macro, epoch)\n",
    "        writer.add_scalar('AUC/val', val_auc_macro, epoch)\n",
    "        writer.add_scalar('LR', current_lr, epoch)\n",
    "        \n",
    "        # Print\n",
    "        print(f\"\\n📊 Results:\")\n",
    "        print(f\"  Train: Loss={train_loss:.4f}, AUC={train_auc_macro:.4f}\")\n",
    "        print(f\"  Val:   Loss={val_loss:.4f}, AUC={val_auc_macro:.4f}\")\n",
    "        print(f\"  Time: {epoch_time/60:.1f}min | Total: {total_elapsed:.2f}hrs\")\n",
    "        \n",
    "        # Compare with previous model\n",
    "        improvement_vs_128 = val_auc_macro - 0.8024  # Your 128x128 val AUC\n",
    "        print(f\"  vs 128x128: {improvement_vs_128:+.4f} ({improvement_vs_128*100:+.2f}%)\")\n",
    "        \n",
    "        if epoch > 0:\n",
    "            avg_time = (time.time() - training_start_time) / (epoch + 1)\n",
    "            eta = (avg_time * (NUM_EPOCHS - epoch - 1)) / 3600\n",
    "            print(f\"  ETA: {eta:.2f} hours\")\n",
    "        \n",
    "        # Save best\n",
    "        if val_auc_macro > best_val_auc:\n",
    "            best_val_auc = val_auc_macro\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_hires.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_auc_macro': val_auc_macro,\n",
    "                'per_class_auc': per_class_auc,\n",
    "                'config': config_hires\n",
    "            }, f'experiments/{experiment_name_hires}/best_model.pth')\n",
    "            print(f\"\\n  ✅ Best model saved! AUC: {best_val_auc:.4f}\")\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"\\n  ⏳ No improvement ({early_stop_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
    "        \n",
    "        if early_stop_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\n⚠️ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - training_start_time\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✅ 224x224 TRAINING COMPLETE!\")\n",
    "    print(f\"Best AUC: {best_val_auc:.4f}\")\n",
    "    print(f\"Time: {total_time/3600:.2f} hours\")\n",
    "    print(f\"Improvement over 128x128: {(best_val_auc-0.8024)*100:+.2f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    writer.close()\n",
    "    return best_val_auc\n",
    "\n",
    "# START TRAINING\n",
    "print(f\"\\n⏰ Starting: {time.strftime('%I:%M %p IST')}\")\n",
    "print(f\"⏰ Expected: ~{time.strftime('%I:%M %p', time.localtime(time.time() + 86400))} IST (tomorrow)\")\n",
    "\n",
    "best_auc_hires = train_model_hires()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7091f-a424-4572-8cc0-b9bff74e40f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
