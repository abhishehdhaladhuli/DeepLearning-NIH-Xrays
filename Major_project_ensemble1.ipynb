{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1113f468-9b9d-44bb-bd13-d857843e2bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet_pytorch in c:\\users\\91850\\anaconda3\\lib\\site-packages (0.7.1)\n",
      "✅ Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pytorch-grad-cam (from versions: none)\n",
      "ERROR: No matching distribution found for pytorch-grad-cam\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# SECTION 1 – IMPORT & CONFIG\n",
    "# ======================================================================\n",
    "\n",
    "!pip install efficientnet_pytorch pytorch-grad-cam scikit-learn matplotlib tqdm\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.cuda.amp import autocast\n",
    "import numpy as np, json, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b635ae-0e1a-4330-ba19-b4c4f831ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# SECTION 2 – MODEL DEFINITION\n",
    "# ======================================================================\n",
    "\n",
    "class ChestXrayClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=14, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.backbone = EfficientNet.from_name('efficientnet-b0')\n",
    "        num_features = self.backbone._fc.in_features\n",
    "        # Same head as during training (Dropout + Linear)\n",
    "        self.backbone._fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0c666bb-6fdb-429c-8a6b-224691ced7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading experiments/chest_xray_20251022_093109/best_model.pth\n",
      "Loading experiments/chest_xray_224x224_20251023_101202/best_model.pth\n",
      "✅ 128×128 Val AUC = 0.8024\n",
      "✅ 224×224 Val AUC = 0.8265\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# SECTION 3 – LOAD BOTH TRAINED MODELS\n",
    "# ======================================================================\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    print(f\"Loading {path}\")\n",
    "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
    "    model = ChestXrayClassifier().to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return model, ckpt\n",
    "\n",
    "model128_path = \"experiments/chest_xray_20251022_093109/best_model.pth\"\n",
    "model224_path = \"experiments/chest_xray_224x224_20251023_101202/best_model.pth\"\n",
    "\n",
    "model128, ckpt128 = load_checkpoint(model128_path)\n",
    "model224, ckpt224 = load_checkpoint(model224_path)\n",
    "\n",
    "print(f\"✅ 128×128 Val AUC = {ckpt128['val_auc_macro']:.4f}\")\n",
    "print(f\"✅ 224×224 Val AUC = {ckpt224['val_auc_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3d6d4e-7b1b-4874-ab4e-faeb569f00ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded test split: 25,596 images.\n",
      "✅ test_loader ready (800 batches of 32)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# REBUILD TEST DATASET AND TEST LOADER (224×224 images)\n",
    "# ======================================================================\n",
    "\n",
    "import os, pandas as pd, numpy as np\n",
    "from albumentations import Compose, Resize, Normalize\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "extract_path = \"./chest_xray_data/\"     # update if different\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1️⃣  Define dataset class (same as during training)\n",
    "# ----------------------------------------------------\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None, num_classes=14):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "        self.pathologies = [\n",
    "            'Atelectasis', 'Consolidation', 'Infiltration', \n",
    "            'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis',\n",
    "            'Effusion', 'Pneumonia', 'Pleural_Thickening', \n",
    "            'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['Image Index']\n",
    "        # possible folders\n",
    "        candidates = [os.path.join(self.img_dir, 'images', img_name)]\n",
    "        for i in range(1, 13):\n",
    "            candidates.append(os.path.join(self.img_dir, f'images_{i:03d}', 'images', img_name))\n",
    "        path = next((p for p in candidates if os.path.exists(p)), None)\n",
    "        if path is None: raise FileNotFoundError(img_name)\n",
    "        img = np.array(Image.open(path).convert(\"RGB\"))\n",
    "        labels_str = self.dataframe.iloc[idx]['Finding Labels']\n",
    "        y = np.zeros(len(self.pathologies), dtype=np.float32)\n",
    "        if labels_str != \"No Finding\":\n",
    "            for i, d in enumerate(self.pathologies):\n",
    "                if d in labels_str: y[i] = 1.0\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "        return img, torch.tensor(y)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2️⃣  Load DataFrame and build test split\n",
    "# ----------------------------------------------------\n",
    "data_entry = pd.read_csv(os.path.join(extract_path, \"Data_Entry_2017.csv\"))\n",
    "test_list_path = os.path.join(extract_path, \"test_list.txt\")\n",
    "\n",
    "if os.path.exists(test_list_path):\n",
    "    test_images = pd.read_csv(test_list_path, header=None)[0].tolist()\n",
    "    test_data = data_entry[data_entry[\"Image Index\"].isin(test_images)].reset_index(drop=True)\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    _, test_data = train_test_split(data_entry, test_size=0.3, random_state=42)\n",
    "print(f\"✅ Loaded test split: {len(test_data):,} images.\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3️⃣  Define transforms (224×224 normalization)\n",
    "# ----------------------------------------------------\n",
    "val_transforms = Compose([\n",
    "    Resize(224, 224),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4️⃣  Build DataLoader\n",
    "# ----------------------------------------------------\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "test_dataset = ChestXrayDataset(test_data, img_dir=extract_path, transform=val_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"✅ test_loader ready ({len(test_loader)} batches of {BATCH_SIZE})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f92c15-64c8-46ac-8620-cbfaa5a12fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91850\\AppData\\Local\\Temp\\ipykernel_21316\\798882183.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Predicting: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [1:06:54<00:00,  5.02s/it]\n",
      "Predicting: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [1:14:45<00:00,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes – 128:(25596, 14), 224:(25596, 14), Labels:(25596, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# SECTION 4 – COLLECT PREDICTIONS\n",
    "# ======================================================================\n",
    "\n",
    "def get_model_predictions(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(loader, desc=\"Predicting\"):\n",
    "            imgs = imgs.to(device)\n",
    "            with autocast():\n",
    "                out = model(imgs)\n",
    "                probs = torch.sigmoid(out).cpu().numpy()\n",
    "            preds.append(probs)\n",
    "            labels.append(lbls.numpy())\n",
    "    return np.vstack(preds), np.vstack(labels)\n",
    "\n",
    "pred128, labels = get_model_predictions(model128, test_loader)\n",
    "pred224, _ = get_model_predictions(model224, test_loader)\n",
    "\n",
    "print(f\"Shapes – 128:{pred128.shape}, 224:{pred224.shape}, Labels:{labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3635459d-070d-4ec4-8249-0118c8a2f60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (macro): 0.7870 | AUC (weighted): 0.7659\n",
      "Acc: 0.7374 | F1 macro: 0.2769 | F1 weighted: 0.3422\n",
      "✅ Saved final ensemble metrics → experiments/final_ensemble_results.json\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# SECTION 5 – ENSEMBLE EVALUATION\n",
    "# ======================================================================\n",
    "\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    auc_macro = roc_auc_score(y_true, y_pred, average=\"macro\")\n",
    "    auc_weighted = roc_auc_score(y_true, y_pred, average=\"weighted\")\n",
    "    binary = (y_pred >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true.flatten(), binary.flatten())\n",
    "    f1m = f1_score(y_true, binary, average=\"macro\")\n",
    "    f1w = f1_score(y_true, binary, average=\"weighted\")\n",
    "    print(f\"AUC (macro): {auc_macro:.4f} | AUC (weighted): {auc_weighted:.4f}\")\n",
    "    print(f\"Acc: {acc:.4f} | F1 macro: {f1m:.4f} | F1 weighted: {f1w:.4f}\")\n",
    "    return auc_macro, auc_weighted, acc, f1m, f1w\n",
    "\n",
    "# Weighted fusion – favor 224×224 (0.6)\n",
    "w1, w2 = 0.4, 0.6\n",
    "ensemble_preds = np.clip(w1*pred128 + w2*pred224, 0, 1)\n",
    "ensemble_metrics = evaluate_preds(labels, ensemble_preds)\n",
    "\n",
    "# Save\n",
    "res = {\n",
    "    \"Model_128x128\": float(ckpt128[\"val_auc_macro\"]),\n",
    "    \"Model_224x224\": float(ckpt224[\"val_auc_macro\"]),\n",
    "    \"Ensemble_AUC_macro\": ensemble_metrics[0],\n",
    "    \"AUC_weighted\": ensemble_metrics[1],\n",
    "    \"Accuracy\": ensemble_metrics[2],\n",
    "    \"F1_macro\": ensemble_metrics[3],\n",
    "    \"F1_weighted\": ensemble_metrics[4],\n",
    "}\n",
    "with open(\"experiments/final_ensemble_results.json\", \"w\") as f:\n",
    "    json.dump(res, f, indent=4)\n",
    "print(\"✅ Saved final ensemble metrics → experiments/final_ensemble_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06a1ca22-832f-4e9f-9ffa-0a445f875f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import cv2, random, matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# choose target convolution layer\n",
    "target_layers = [model224.backbone._conv_head]\n",
    "\n",
    "# NEW correct syntax (no use_cuda)\n",
    "cam = GradCAM(model=model224, target_layers=target_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4acd628-7495-424e-b5d3-88ec44aa40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradcam(image_path, predicted_class_idx):\n",
    "    img = cv2.imread(image_path)\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "    input_tensor = torch.tensor(rgb_img/255.).permute(2,0,1).unsqueeze(0).float().to(device)\n",
    "\n",
    "    targets = [ClassifierOutputTarget(predicted_class_idx)]\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    visualization = show_cam_on_image(rgb_img/255., grayscale_cam, use_rgb=True)\n",
    "\n",
    "    plt.imshow(visualization)\n",
    "    plt.title(f\"Grad‑CAM (class {predicted_class_idx})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c468873b-195e-4a75-8edc-a7f73a01c9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 112,120 total images, sampling 20 for Grad‑CAM...\n",
      "✅ Saved 20 Grad‑CAM heatmaps → gradcam_visuals/\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# FIX - LOOK FOR NIH IMAGES IN ALL SUBDIRECTORIES\n",
    "# ======================================================================\n",
    "import os, glob, random, cv2\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# gather all .png or .jpg files from every subfolder like images_001/.../images\n",
    "image_paths = []\n",
    "for i in range(1, 13):\n",
    "    folder = f\"./chest_xray_data/images_{i:03d}/images\"\n",
    "    if os.path.exists(folder):\n",
    "        image_paths.extend(glob.glob(os.path.join(folder, \"*.png\")))\n",
    "        image_paths.extend(glob.glob(os.path.join(folder, \"*.jpg\")))\n",
    "\n",
    "# sample 20 random images\n",
    "samples = random.sample(image_paths, 20)\n",
    "save_dir = \"gradcam_visuals\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Found {len(image_paths):,} total images, sampling 20 for Grad‑CAM...\")\n",
    "\n",
    "# run Grad‑CAM visualization save loop\n",
    "for path in samples:\n",
    "    fname = os.path.basename(path)\n",
    "    img = cv2.imread(path)\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    rgb_img = cv2.resize(rgb_img, (224,224))\n",
    "    input_tensor = torch.tensor(rgb_img/255.).permute(2,0,1).unsqueeze(0).float().to(device)\n",
    "    targets = [ClassifierOutputTarget(10)]  # example class index: Cardiomegaly\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "    vis = show_cam_on_image(rgb_img/255., grayscale_cam, use_rgb=True)\n",
    "    cv2.imwrite(f\"{save_dir}/gradcam_{fname}\", cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "print(f\"✅ Saved 20 Grad‑CAM heatmaps → {save_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "933e99e9-911d-4a99-b05b-88482c62a639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Labeled gradcam visuals updated for presentation.\n"
     ]
    }
   ],
   "source": [
    "# Example snippet to overlay label text on saved Grad‑CAM images\n",
    "import cv2, os\n",
    "for img in os.listdir(\"gradcam_visuals\"):\n",
    "    path = f\"gradcam_visuals/{img}\"\n",
    "    vis = cv2.imread(path)\n",
    "    label_text = \"Cardiomegaly  (Pred: Positive)\"\n",
    "    cv2.putText(vis, label_text, (15, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)\n",
    "    cv2.imwrite(path, vis)\n",
    "print(\"✅ Labeled gradcam visuals updated for presentation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64418d6d-e167-4d85-9a8d-b07ac39c0809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
